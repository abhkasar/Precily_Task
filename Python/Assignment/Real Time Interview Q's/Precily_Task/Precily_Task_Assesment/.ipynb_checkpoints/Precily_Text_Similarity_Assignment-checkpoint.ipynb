{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4dfe10",
   "metadata": {},
   "source": [
    "# Importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51598ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings, string\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader as api\n",
    "from nltk import word_tokenize\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efe034",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcf214f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challenges tv viewing the number of ...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap boss arrested over drug find rap mogul mar...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burn-out worries robinson england coach...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hearts of oak 3-2 cotonsport hearts of oak set...</td>\n",
       "      <td>redford s vision of sundance despite sporting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens with victory in la amelie maure...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challenges tv viewing the number of ...   \n",
       "1  rap boss arrested over drug find rap mogul mar...   \n",
       "2  player burn-out worries robinson england coach...   \n",
       "3  hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "4  sir paul rocks super bowl crowds sir paul mcca...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double in glasgow britain s jaso...  \n",
       "1  amnesty chief laments war failure the lack of ...  \n",
       "2  hanks greeted at wintry premiere hollywood sta...  \n",
       "3  redford s vision of sundance despite sporting ...  \n",
       "4  mauresmo opens with victory in la amelie maure...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Precily_Text_Similarity.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1ce99",
   "metadata": {},
   "source": [
    "# Text Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6949af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "df['text1'] = df['text1'].apply(lambda x: stem_words(x))\n",
    "df['text2'] = df['text2'].apply(lambda x: stem_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080a534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "df['text1'] = df['text1'].apply(lambda x: lemmatize_words(x))\n",
    "df['text2'] = df['text2'].apply(lambda x: lemmatize_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f8e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english') and not word.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1646fced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b093980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text1'] = df['text1'].apply(lambda x: text_process(x))\n",
    "df['text2'] = df['text2'].apply(lambda x: text_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1685298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e075dd",
   "metadata": {},
   "source": [
    "## Jaccard Similarity Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621fd0ae",
   "metadata": {},
   "source": [
    "The Jaccard similarity index measures the similarity between two sets of data. It can range from 0 to 1. The higher the number, the more similar the two sets of data.\n",
    "\n",
    "The Jaccard similarity index is calculated as:\n",
    "\n",
    "Jaccard Similarity = (number of observations in both sets) / (number in either set)\n",
    "\n",
    "Or, written in notation form:\n",
    "\n",
    "J(A, B) = |A∩B| / |A∪B|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c3a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(a,b):\n",
    "    intersection = set(a).intersection(set(b))\n",
    "    union = set(a).union(set(b))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "523092ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7222222222222222"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(df['text1'][0],df['text2'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c805c",
   "metadata": {},
   "source": [
    "# Text Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6ce1c",
   "metadata": {},
   "source": [
    "### Count Vectorizer - TF(Term Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9831df",
   "metadata": {},
   "source": [
    "We'll convert each message, represented as a list of tokens, into a vector that machine learning models can understand.\n",
    "\n",
    "We'll do that in three steps using the bag-of-words model:\n",
    "\n",
    "1. Count how many times does a word occur in each message (Known as term frequency)\n",
    "\n",
    "2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352b474",
   "metadata": {},
   "source": [
    "Each vector will have as many dimensions as there are unique words in the SMS corpus.  We will first use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b595761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=<function text_process at 0x000002798A903280>)\n",
      "CountVectorizer(analyzer=<function text_process at 0x000002798A903280>)\n"
     ]
    }
   ],
   "source": [
    "bow_transformer1 = CountVectorizer(analyzer=text_process).fit(df['text1'])\n",
    "bow_transformer2 = CountVectorizer(analyzer=text_process).fit(df['text2'])\n",
    "print(bow_transformer1)\n",
    "print(bow_transformer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9af2b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 38)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_transformer1.vocabulary_), len(bow_transformer2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e79570",
   "metadata": {},
   "source": [
    "Let's consider the 4th message of the text1 column of the given dataset and generate bag of words along with their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f611fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t167\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t95\n",
      "  (0, 12)\t17\n",
      "  (0, 13)\t32\n",
      "  (0, 14)\t23\n",
      "  (0, 15)\t88\n",
      "  (0, 16)\t24\n",
      "  (0, 17)\t22\n",
      "  (0, 18)\t23\n",
      "  (0, 19)\t56\n",
      "  (0, 20)\t3\n",
      "  (0, 21)\t19\n",
      "  (0, 22)\t39\n",
      "  (0, 23)\t31\n",
      "  (0, 24)\t55\n",
      "  (0, 25)\t70\n",
      "  (0, 26)\t12\n",
      "  (0, 27)\t4\n",
      "  (0, 28)\t55\n",
      "  (0, 29)\t35\n",
      "  (0, 30)\t72\n",
      "  (0, 31)\t29\n",
      "  (0, 32)\t3\n",
      "  (0, 33)\t13\n",
      "  (0, 34)\t1\n",
      "  (0, 35)\t6\n",
      "  (0, 36)\t2\n"
     ]
    }
   ],
   "source": [
    "text4 = df['text1'][3]\n",
    "bow4 = bow_transformer1.transform([text4])\n",
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 38)\n"
     ]
    }
   ],
   "source": [
    "bow_text1 = bow_transformer1.transform(df['text1'])\n",
    "print(bow_text1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21291499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 38)\n"
     ]
    }
   ],
   "source": [
    "bow_text2 = bow_transformer2.transform(df['text2'])\n",
    "print(bow_text2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a45a9f",
   "metadata": {},
   "source": [
    "## TF-IDF Transformer(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d61236",
   "metadata": {},
   "source": [
    "TF-IDF stands for *term frequency-inverse document frequency*, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb60086",
   "metadata": {},
   "source": [
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "*TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).*\n",
    "\n",
    "**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "*IDF(t) = log_e(Total number of documents / Number of documents with term t in it).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408fb8ff",
   "metadata": {},
   "source": [
    "#### Using Sklearn's TFIDF Transformer to transform the entire bag of words corpus into TFIDF corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d74dc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer1 = TfidfTransformer().fit(bow_text1)\n",
    "tfidf_transformer2 = TfidfTransformer().fit(bow_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cceddc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 36)\t0.013055846780139978\n",
      "  (0, 35)\t0.022250077435161244\n",
      "  (0, 34)\t0.004138953140290266\n",
      "  (0, 33)\t0.04820850110951602\n",
      "  (0, 32)\t0.011128746445899422\n",
      "  (0, 31)\t0.10754204093661267\n",
      "  (0, 30)\t0.2670009292219349\n",
      "  (0, 29)\t0.1297921183717739\n",
      "  (0, 28)\t0.20395904315564473\n",
      "  (0, 27)\t0.0214428335884421\n",
      "  (0, 26)\t0.04450015487032249\n",
      "  (0, 25)\t0.2595842367435478\n",
      "  (0, 24)\t0.20395904315564473\n",
      "  (0, 23)\t0.11495873341499975\n",
      "  (0, 22)\t0.14462550332854807\n",
      "  (0, 21)\t0.07088246164593452\n",
      "  (0, 20)\t0.012989252523926654\n",
      "  (0, 19)\t0.20766738939483825\n",
      "  (0, 18)\t0.08529196350145143\n",
      "  (0, 17)\t0.0815836172622579\n",
      "  (0, 16)\t0.08900030974064498\n",
      "  (0, 15)\t0.3263344690490316\n",
      "  (0, 14)\t0.08529196350145143\n",
      "  (0, 13)\t0.11866707965419329\n",
      "  (0, 12)\t0.06304188606629019\n",
      "  (0, 11)\t0.3522928927233863\n",
      "  (0, 10)\t0.011089971430165385\n",
      "  (0, 9)\t0.010339242840822743\n",
      "  (0, 6)\t0.008505999327078297\n",
      "  (0, 4)\t0.009006977779888707\n",
      "  (0, 0)\t0.6192938219453212\n"
     ]
    }
   ],
   "source": [
    "tfidf4 = tfidf_transformer1.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fab69022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 38)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf4.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe65362",
   "metadata": {},
   "source": [
    "#### To transform the entire bag of words corpus into TFIDF corpus at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f032b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text1 = tfidf_transformer1.transform(bow_text1)\n",
    "tfidf_text2 = tfidf_transformer2.transform(bow_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e83c0a",
   "metadata": {},
   "source": [
    "### Checking out the various features of the TFIDF sparse matrices of the 2 text columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4458ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of non-zero values in TFIDF of text 1: 85549\n",
      "Amount of non-zero values in TFIDF of text 2: 85711\n"
     ]
    }
   ],
   "source": [
    "print(\"Amount of non-zero values in TFIDF of text 1:\",tfidf_text1.nnz)\n",
    "print(\"Amount of non-zero values in TFIDF of text 2:\",tfidf_text2.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66c02195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TFIDF of text 1: (3000, 38)\n",
      "Shape of TFIDF of text 2: (3000, 38)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of TFIDF of text 1:\",tfidf_text1.shape)\n",
    "print(\"Shape of TFIDF of text 2:\",tfidf_text2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e9a20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of text 1: 75.04%\n",
      "Sparsity of text 2: 75.19%\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparsity of text 1:\",str(np.round((tfidf_text1.nnz/(tfidf_text1.shape[0]*tfidf_text1.shape[1]))*100,2)) + '%')\n",
    "print(\"Sparsity of text 2:\",str(np.round((tfidf_text2.nnz/(tfidf_text2.shape[0]*tfidf_text2.shape[1]))*100,2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34317a26",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9be12",
   "metadata": {},
   "source": [
    "The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281f41a",
   "metadata": {},
   "source": [
    "### Evaluating the cosine similarity between any two TFIDF vectors in order to determine the degree of lexical and semantic resemblance, closeness or similarity of the two vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ccc49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.9785293861463167, 0.9887675564206015, 0.988...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.970073371768313, 0.9818744094402776, 0.9865...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.9643083804078563, 0.9701381455232274, 0.962...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.9759967531076594, 0.982251639784895, 0.9806...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.9834646778322373, 0.9882872871600276, 0.994...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "1  [0.9785293861463167, 0.9887675564206015, 0.988...\n",
       "2  [0.970073371768313, 0.9818744094402776, 0.9865...\n",
       "3  [0.9643083804078563, 0.9701381455232274, 0.962...\n",
       "4  [0.9759967531076594, 0.982251639784895, 0.9806...\n",
       "5  [0.9834646778322373, 0.9882872871600276, 0.994..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity = [[]]\n",
    "for i in range(tfidf_text1.shape[0]):\n",
    "    cos_similarity.append(cosine_similarity(tfidf_text1[i],tfidf_text2))\n",
    "cos_similarity = pd.DataFrame(cos_similarity)\n",
    "cos_similarity.drop(index=cos_similarity.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "cos_similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6c532bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9c68e",
   "metadata": {},
   "source": [
    "## Generating the cosine similarity matrix consisting of the cosine similarity scores of each and every text present in both the columns 'text1' and 'text2'\n",
    "### It is of dimensions 3000X3000 as each text paragraph in 'text1' column is mapped with each of the 3000 text paragraphs of the 'text2' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64264209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.978529</td>\n",
       "      <td>0.970073</td>\n",
       "      <td>0.964308</td>\n",
       "      <td>0.975997</td>\n",
       "      <td>0.983465</td>\n",
       "      <td>0.974899</td>\n",
       "      <td>0.975446</td>\n",
       "      <td>0.981171</td>\n",
       "      <td>0.974635</td>\n",
       "      <td>0.971818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976139</td>\n",
       "      <td>0.977350</td>\n",
       "      <td>0.982670</td>\n",
       "      <td>0.970279</td>\n",
       "      <td>0.969073</td>\n",
       "      <td>0.979111</td>\n",
       "      <td>0.982001</td>\n",
       "      <td>0.961637</td>\n",
       "      <td>0.979260</td>\n",
       "      <td>0.975768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.988768</td>\n",
       "      <td>0.981874</td>\n",
       "      <td>0.970138</td>\n",
       "      <td>0.982252</td>\n",
       "      <td>0.988287</td>\n",
       "      <td>0.981370</td>\n",
       "      <td>0.978910</td>\n",
       "      <td>0.989982</td>\n",
       "      <td>0.980058</td>\n",
       "      <td>0.979575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>0.985446</td>\n",
       "      <td>0.983493</td>\n",
       "      <td>0.978564</td>\n",
       "      <td>0.971599</td>\n",
       "      <td>0.977041</td>\n",
       "      <td>0.990116</td>\n",
       "      <td>0.969929</td>\n",
       "      <td>0.981814</td>\n",
       "      <td>0.987708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.988882</td>\n",
       "      <td>0.986596</td>\n",
       "      <td>0.962266</td>\n",
       "      <td>0.980612</td>\n",
       "      <td>0.994628</td>\n",
       "      <td>0.977163</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.986011</td>\n",
       "      <td>0.980424</td>\n",
       "      <td>0.985989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982220</td>\n",
       "      <td>0.980308</td>\n",
       "      <td>0.975918</td>\n",
       "      <td>0.981216</td>\n",
       "      <td>0.977847</td>\n",
       "      <td>0.976986</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.960132</td>\n",
       "      <td>0.980092</td>\n",
       "      <td>0.977371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.984609</td>\n",
       "      <td>0.973072</td>\n",
       "      <td>0.960169</td>\n",
       "      <td>0.976659</td>\n",
       "      <td>0.985454</td>\n",
       "      <td>0.973015</td>\n",
       "      <td>0.982949</td>\n",
       "      <td>0.994004</td>\n",
       "      <td>0.984009</td>\n",
       "      <td>0.982400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988786</td>\n",
       "      <td>0.983359</td>\n",
       "      <td>0.991103</td>\n",
       "      <td>0.984215</td>\n",
       "      <td>0.978290</td>\n",
       "      <td>0.985069</td>\n",
       "      <td>0.983268</td>\n",
       "      <td>0.969183</td>\n",
       "      <td>0.984444</td>\n",
       "      <td>0.984504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991294</td>\n",
       "      <td>0.981296</td>\n",
       "      <td>0.956340</td>\n",
       "      <td>0.986461</td>\n",
       "      <td>0.989491</td>\n",
       "      <td>0.980954</td>\n",
       "      <td>0.970051</td>\n",
       "      <td>0.984327</td>\n",
       "      <td>0.974304</td>\n",
       "      <td>0.979598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974879</td>\n",
       "      <td>0.974234</td>\n",
       "      <td>0.981270</td>\n",
       "      <td>0.971890</td>\n",
       "      <td>0.969613</td>\n",
       "      <td>0.974051</td>\n",
       "      <td>0.984597</td>\n",
       "      <td>0.962306</td>\n",
       "      <td>0.977571</td>\n",
       "      <td>0.974417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         0         0         0         0         0         0  \\\n",
       "0  0.978529  0.970073  0.964308  0.975997  0.983465  0.974899  0.975446   \n",
       "1  0.988768  0.981874  0.970138  0.982252  0.988287  0.981370  0.978910   \n",
       "2  0.988882  0.986596  0.962266  0.980612  0.994628  0.977163  0.977388   \n",
       "3  0.984609  0.973072  0.960169  0.976659  0.985454  0.973015  0.982949   \n",
       "4  0.991294  0.981296  0.956340  0.986461  0.989491  0.980954  0.970051   \n",
       "\n",
       "          0         0         0  ...         0         0         0         0  \\\n",
       "0  0.981171  0.974635  0.971818  ...  0.976139  0.977350  0.982670  0.970279   \n",
       "1  0.989982  0.980058  0.979575  ...  0.981408  0.985446  0.983493  0.978564   \n",
       "2  0.986011  0.980424  0.985989  ...  0.982220  0.980308  0.975918  0.981216   \n",
       "3  0.994004  0.984009  0.982400  ...  0.988786  0.983359  0.991103  0.984215   \n",
       "4  0.984327  0.974304  0.979598  ...  0.974879  0.974234  0.981270  0.971890   \n",
       "\n",
       "          0         0         0         0         0         0  \n",
       "0  0.969073  0.979111  0.982001  0.961637  0.979260  0.975768  \n",
       "1  0.971599  0.977041  0.990116  0.969929  0.981814  0.987708  \n",
       "2  0.977847  0.976986  0.986111  0.960132  0.980092  0.977371  \n",
       "3  0.978290  0.985069  0.983268  0.969183  0.984444  0.984504  \n",
       "4  0.969613  0.974051  0.984597  0.962306  0.977571  0.974417  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix = pd.DataFrame()\n",
    "for i in range(cos_similarity.shape[0]):\n",
    "    cosine_similarity_matrix = pd.concat([cosine_similarity_matrix,(pd.DataFrame(cos_similarity.iloc[i].values.tolist()).T)],axis=1)\n",
    "cosine_similarity_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29092326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e56de7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    707\n",
       "0    707\n",
       "0    707\n",
       "0    614\n",
       "0    707\n",
       "    ... \n",
       "0    586\n",
       "0    707\n",
       "0    614\n",
       "0    707\n",
       "0    707\n",
       "Length: 3000, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfb93ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881484386605357"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix.iloc[707].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc1da9",
   "metadata": {},
   "source": [
    "## Leveraging Google News Word 2 Vector library present in gensim.downloader module to generate a huge inbuilt word vocabulary which can be used to check for semantic similarity between the two texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1472663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a459b",
   "metadata": {},
   "source": [
    "## Function to calculate the semantic similarity between the two text documents based on the concept of word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04d21cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = []\n",
    "\n",
    "for idx in df.index:\n",
    "    t1 = df['text1'][idx]\n",
    "    t2 = df['text2'][idx]\n",
    "    \n",
    "    if t1 == t2:\n",
    "        similarity.append(1)\n",
    "    else:\n",
    "        t1_words = word_tokenize(t1)\n",
    "        t2_words = word_tokenize(t2)\n",
    "        vocab = wv.vocab\n",
    "        \n",
    "        if len(t1_words and t2_words) == 0:\n",
    "            similarity.append(0)\n",
    "        else:\n",
    "            for word in t1_words.copy():\n",
    "                if word not in vocab:\n",
    "                    t1_words.remove(word)\n",
    "            for word in t2_words.copy():\n",
    "                if word not in vocab:\n",
    "                    t2_words.remove(word)\n",
    "            similarity.append(wv.n_similarity(t1_words,t2_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05354e3",
   "metadata": {},
   "source": [
    "Generating a dataframe for the similarity scores for every pair of texts present in each row of the given dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bc55394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.738640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.667202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.775820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.865341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.738640\n",
       "1  0.667202\n",
       "2  0.775820\n",
       "3  0.658866\n",
       "4  0.865341"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = pd.DataFrame(similarity)\n",
    "similarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "764e2f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challeng tv view number european bro...</td>\n",
       "      <td>garden win doubl glasgow britain jason garden ...</td>\n",
       "      <td>0.738640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap bos arrest drug find rap mogul marion suge...</td>\n",
       "      <td>amnesti chief lament war failur lack public ou...</td>\n",
       "      <td>0.667202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burnout worri robinson england coach an...</td>\n",
       "      <td>hank greet wintri premier hollywood star tom h...</td>\n",
       "      <td>0.775820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heart oak cotonsport heart oak set ghanaian co...</td>\n",
       "      <td>redford vision sundanc despit sport corduroy c...</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rock super bowl crowd sir paul mccart...</td>\n",
       "      <td>mauresmo open victori la ameli mauresmo maria ...</td>\n",
       "      <td>0.865341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challeng tv view number european bro...   \n",
       "1  rap bos arrest drug find rap mogul marion suge...   \n",
       "2  player burnout worri robinson england coach an...   \n",
       "3  heart oak cotonsport heart oak set ghanaian co...   \n",
       "4  sir paul rock super bowl crowd sir paul mccart...   \n",
       "\n",
       "                                               text2         0  \n",
       "0  garden win doubl glasgow britain jason garden ...  0.738640  \n",
       "1  amnesti chief lament war failur lack public ou...  0.667202  \n",
       "2  hank greet wintri premier hollywood star tom h...  0.775820  \n",
       "3  redford vision sundanc despit sport corduroy c...  0.658866  \n",
       "4  mauresmo open victori la ameli mauresmo maria ...  0.865341  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df,similarity],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f90b315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text1','text2','Similarity Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8bfcaa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>broadband challeng tv view number european bro...</td>\n",
       "      <td>garden win doubl glasgow britain jason garden ...</td>\n",
       "      <td>0.738640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rap bos arrest drug find rap mogul marion suge...</td>\n",
       "      <td>amnesti chief lament war failur lack public ou...</td>\n",
       "      <td>0.667202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>player burnout worri robinson england coach an...</td>\n",
       "      <td>hank greet wintri premier hollywood star tom h...</td>\n",
       "      <td>0.775820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heart oak cotonsport heart oak set ghanaian co...</td>\n",
       "      <td>redford vision sundanc despit sport corduroy c...</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir paul rock super bowl crowd sir paul mccart...</td>\n",
       "      <td>mauresmo open victori la ameli mauresmo maria ...</td>\n",
       "      <td>0.865341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  broadband challeng tv view number european bro...   \n",
       "1  rap bos arrest drug find rap mogul marion suge...   \n",
       "2  player burnout worri robinson england coach an...   \n",
       "3  heart oak cotonsport heart oak set ghanaian co...   \n",
       "4  sir paul rock super bowl crowd sir paul mccart...   \n",
       "\n",
       "                                               text2  Similarity Score  \n",
       "0  garden win doubl glasgow britain jason garden ...          0.738640  \n",
       "1  amnesti chief lament war failur lack public ou...          0.667202  \n",
       "2  hank greet wintri premier hollywood star tom h...          0.775820  \n",
       "3  redford vision sundanc despit sport corduroy c...          0.658866  \n",
       "4  mauresmo open victori la ameli mauresmo maria ...          0.865341  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f571a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('similarity_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1251cd",
   "metadata": {},
   "source": [
    "## Saving and loading the cosine similarity model within the joblib library in order to use it during the time of model deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e739e9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cosine_similarity,'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8206031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
